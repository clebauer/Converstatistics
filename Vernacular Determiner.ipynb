{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program Sketch\n",
    "\n",
    "**Goal**: To determine the unique set of words that two given people use together (that a given person rarely uses with other people).\n",
    "\n",
    "**Super High Level**: Find all conversations, scrape JSON for base-case words, scrape rest of JSONs to determine core set of words, pull words on end of frequency chart (1? 2? s.d.)\n",
    "\n",
    "**High Level Task List**:\n",
    "1. Pull messenger data for a given person and convert into a DataFrame.\n",
    "2. Clean up the DataFrame so it can be interacted with easily.\n",
    "3. Parse words in DataFrame for the user to create initial data set of words.\n",
    "4. Repeat this process for 10 other people, to start.\n",
    "5. Determine which words show up in 10/10, 9/10, etc cases.\n",
    "6. Test this on 10 other people, 100 other people, all cases (if time allows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    # TODO: sometimes a message dataframe doesn't have a 'content' column.\n",
    "    \n",
    "    # sort by oldest message\n",
    "    df.sort('timestamp_ms', inplace=True)\n",
    "    \n",
    "    # set all content to lowercase for easier parsing going forward\n",
    "    df['content'] = df['content'].str.lower()\n",
    "    df['content'].fillna(value='', inplace=True)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['timestamp_ms'], unit='ms')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_filename_to_df(file_name):\n",
    "    # open the data\n",
    "    with open(file_name) as f: #\n",
    "        data = json.load(f)\n",
    "        \n",
    "    # pull out messages for dataframe conversion\n",
    "    messages = [x for x in data['messages']]\n",
    "    \n",
    "    df = pd.DataFrame(messages)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def person(df, name):\n",
    "    # this assumes you've already created chat_df\n",
    "    unique_names = df['sender_name'].unique()\n",
    "    if name in unique_names:\n",
    "        return df[df['sender_name'] == name]\n",
    "    else:\n",
    "        #print name, 'isn\\'t in the DataFrame. \\nTry one of these:', ', '.join(unique_names.astype(str))\n",
    "        return pd.DataFrame(columns=test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_this(df, max_limit=100000000, min_limit=0):\n",
    "    len_num = len(person(df, my_name))\n",
    "    # check all of these things in order of 'ew, bye'ness:\n",
    "    # * if Ben is in the conversation\n",
    "    # * if I've said very little in the conversation (less than 100?)\n",
    "    if (('Ben Knight' in set(df['sender_name']))\n",
    "        or (len_num >= max_limit)\n",
    "        or (len_num < min_limit)\n",
    "        ):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordlist(file_name, name, stop_file=False, way='common'):\n",
    "    # Let's parse our data.\n",
    "    chat_df_raw = convert_filename_to_df(file_name)\n",
    "    chat_df = clean_dataframe(chat_df_raw)\n",
    "\n",
    "    # Pull out only my chats\n",
    "    my_chats = person(chat_df, name)\n",
    "\n",
    "    # pick name_word. if it's you, use 'you'. else, use their first name\n",
    "    name_word = name.lower().split()[0] if name != my_name else 'you' \n",
    "    sent_str = '{} sent a'.format(name_word)\n",
    "    \n",
    "    # pull only my sentences out\n",
    "    convos = my_chats[my_chats['content'].apply(lambda x: sent_str not in x)]['content']\n",
    "    \n",
    "    # unicode word list\n",
    "    unicode_list = \" \".join(convos.str.lower()).split()\n",
    "    \n",
    "    # convert each word to a string and remove punctuation.\n",
    "    sample_words = [x.encode('UTF8').translate(None, string.punctuation) for x in unicode_list if ('http' not in x) and (not x.startswith('www'))]\n",
    "    \n",
    "    # counter set of those words, removing empty spaces\n",
    "    base_words = Counter([w for w in sample_words if w != ''])\n",
    "    \n",
    "    if way == 'common':\n",
    "        return base_words\n",
    "    elif way == 'count':\n",
    "        return [(k, v) for k, v in base_words.iteritems()]\n",
    "    elif way == 'percent':\n",
    "        return [(k, round(1000.0*v/sum(base_words.values()),5)) for k, v in base_words.iteritems()]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = main_file\n",
    "name = my_name\n",
    "stop_file=True\n",
    "way='common'\n",
    "\n",
    "# Let's parse our data.\n",
    "chat_df_raw = convert_filename_to_df(file_name)\n",
    "chat_df = clean_dataframe(chat_df_raw)\n",
    "\n",
    "# Pull out only my chats\n",
    "my_chats = person(chat_df, name)\n",
    "\n",
    "# pick name_word. if it's you, use 'you'. else, use their first name\n",
    "name_word = name.lower().split()[0] if name != my_name else 'you' \n",
    "sent_str = '{} sent a'.format(name_word)\n",
    "\n",
    "# pull only my sentences out\n",
    "convos = my_chats[my_chats['content'].apply(lambda x: sent_str not in x)]['content']\n",
    "\n",
    "# unicode word list\n",
    "unicode_list = \" \".join(convos.str.lower()).split()\n",
    "\n",
    "# convert each word to a string and remove punctuation.\n",
    "sample_words = [x.encode('UTF8').translate(None, string.punctuation) for x in unicode_list if ('http' not in x) and (not x.startswith('www'))]\n",
    "\n",
    "# counter set of those words, removing empty spaces\n",
    "base_words = Counter([w for w in sample_words if w != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words2 = [x.encode('UTF8').translate(None, string.punctuation) for x in unicode_list if (x not in stops['word']) and ('http' not in x) and (not x.startswith('www'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_words3 = Counter([w for w in sample_words if (w != '') and (w not in stops2['word'].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops2 = pd.read_csv('stop_file2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_wordlist(main_file, my_name, stops2).most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_name = 'Cassie Beth'\n",
    "main_name = 'Ben Knight'\n",
    "main_file = 'BenKnight/message.json'\n",
    "inbox_url = input(\"Where are the files located\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the Messenger Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull main dataset\n",
    "main_df = convert_filename_to_df(main_file)\n",
    "main_data = generate_wordlist(main_file, main_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each folder of conversations, pull the data.\n",
    "# then, perform some checks to see if we want to use this dataframe or not.\n",
    "#for num_limit in range(0,1001,50):\n",
    "min_limit = 500\n",
    "#max_limit = 500\n",
    "count = {'good':0,'bad':0}\n",
    "\n",
    "for human in os.listdir(inbox_url)[1:]:\n",
    "    file_name = '/'.join([inbox_url, human, 'message.json'])\n",
    "    #print file_name\n",
    "    test_df = convert_filename_to_df(file_name)\n",
    "    if check_this(test_df, min_limit=min_limit):\n",
    "        count['good'] += 1\n",
    "        #print '{}, len: {}'.format(human, len(test_df))\n",
    "    else:\n",
    "        count['bad'] += 1\n",
    "        #print 'Bad: {}'.format(human)\n",
    "\n",
    "#    print 'With message limit {}, {}'.format(num_limit, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(generate_wordlist(file_name, my_name)), type(generate_wordlist(file_name, my_name).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each folder of conversations, pull the data.\n",
    "# then, perform some checks to see if we want to use this dataframe or not.\n",
    "#for num_limit in range(0,1001,50):\n",
    "min_limit = 500\n",
    "#max_limit = 500\n",
    "count = {'good':0,'bad':0}\n",
    "full_tbl = pd.DataFrame(columns=['word'])\n",
    "#mc_count = 100 .most_common(mc_count)\n",
    "\n",
    "for human in os.listdir(inbox_url)[1:]:\n",
    "    file_name = '/'.join([inbox_url, human, 'message.json'])\n",
    "    #print file_name\n",
    "    test_df = convert_filename_to_df(file_name)\n",
    "    if check_this(test_df, min_limit=min_limit):\n",
    "        test_words = [(k, v) for k, v in generate_wordlist(file_name, my_name, way='percent')]\n",
    "        test_tbl = pd.DataFrame(test_words, columns=['word', human.split('_')[0]])\n",
    "        full_tbl = pd.merge(full_tbl, test_tbl, on = 'word', how='outer')\n",
    "        #print '{}, len: {}'.format(human, len(test_df))\n",
    "\n",
    "print 'With message limit {}, {}'.format(num_limit, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tbl = full_tbl.set_index(full_tbl['word']).drop(['word'], axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tbl['total'] = full_tbl.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tbl = full_tbl.sort('total', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = pd.DataFrame(stop_tbl.reset_index()['word']).to_csv('stop_file.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tbl = full_tbl[full_tbl['total'] > full_tbl['total'].mean()]# + full_tbl['total'].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = pd.DataFrame(stop_tbl.reset_index()['word']).to_csv('stop_file2.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun Facts of Learning:\n",
    "#\n",
    "# # number of \"words\" I typed to Ben for the first year we talked.\n",
    "# len([x.translate(None, string.punctuation) for x in sample_words])\n",
    "# > 231,132\n",
    "#\n",
    "# # number of unique words with punctuation\n",
    "# len(set(sample_words))\n",
    "# > 24,259\n",
    "#\n",
    "# # number of unique words, no punctuation!\n",
    "# len(set([x.translate(None, string.punctuation) for x in sample_words]))\n",
    "# > 14,176\n",
    "\n",
    "# Fun Facts:\n",
    "# strip out punctuation\n",
    "# strip out http links\n",
    "\n",
    "# think about:\n",
    "# \\xc3\\xa3\\xc2\\xa1\n",
    "# 3 instead of <3\n",
    "# misspellings\n",
    "# long one sentence, many words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
